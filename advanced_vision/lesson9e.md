---
marp: true
---

<!-- footer: "Advanced Vision Lesson 9" -->

# Advanced Vision

## Lesson 9: Integration of Images, Language, and Robot Control I

Ryuichi Ueda, Chiba Institute of Technology

<br />

<span style="font-size:70%">This work is licensed under a </span>[<span style="font-size:70%">Creative Commons Attribution-ShareAlike 4.0 International License</span>](https://creativecommons.org/licenses/by-sa/4.0/).
![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)

---

<!-- paginate: true -->

## Contents

- Before the main topic
   - PaLM
   - PaLI
- Robot Control using ANNs
    - models before the term VLA (vision-language-action model) appears
        - We'll discuss what happened after the term VLA appeared next time.

---

## PaLM[[Chowdhery2022]](https://arxiv.org/abs/2204.02311)

- Pathways Language Model (Google's large-scale language model)
    - It's composed of a Transformer decoder, so it functions like GPT.
    - As described below, it's used in Google's robot control models.
    - The new version, PaLM 2[[Anil2023]](https://arxiv.org/abs/2305.10403), can handle over 100 languages (multilingual translation is possible).

---
## PaLI[[Chen2022]](https://arxiv.org/abs/2209.06794)

- Pathways Language and Image model
    - Structure: Figure 1 in the paper (ViT + Transformer encoder + Transformer decoder)
        - Word tokens and image feature tokens generated by ViT are input into the Transformer as vectors of the same length.
        - It appears that a cross-attention mechanism is also used (sorry, not investigated).

---

## Controlling a robot using an ANN

---

### Basic concept

- Building an ANN that converts various types of information into motion.
    - Images, sensors
    - Verbal instructions
    - Its own internal state and structure
- Goal: extending the existing vision-language model
    - To enable input of information other than images
    - To enable output of motion

---

### Challenges of controlling a robot using an ANN

- How can we collect training data?
    - There probably isn't any online data like there was with CLIP.
    - <span style="color:red">Humans hired by big companies prepare the data. (It's a kind of distopia?)</span>
    - Of course, a simulator (digital twin) can also be used.
        - Examples of the two methods above: https://www.youtube.com/watch?v=S4tvirlG8sQ
- How to express and output policies (control rules)
    - how to design the format
    - how to output long control sequences

---

- Generalization
    - Able to perform actions outside of those taught
    - Possibly enabled by the presence of language (understanding of meaning)

---

### Robotics Transformer-1 (RT-1) [[Brohan 2022]](https://arxiv.org/abs/2212.06817) ([Video](https://www.youtube.com/watch?v=UuKAp9a6wMs))

- Overview of architecture and input/output: Figure 3 in the paper
    - Universal Sentence Encoder: Outputs parameters from [FiLM](lesson8.html#3)
    - FiLM EfficientNet-B3 and TokenLearner
        - Input: Six time-series images and verbal instructions to the robot
        - Output: 48 tokens with 512 dimensions
- Transformer (decoder)
    - Input: Positional embeddings for the 48 tokens
    - Output: Robot behavior (points in 11-dimensional discrete space)
        - Robot: Mobile manipulator ([Everyday Robots](https://x.company/projects/everyday-robots/))
        - 1-dimensional mode, 7-dimensional arm movement, 3-dimensional position and orientation
        - 3 Hz

---
### RT-1 training data (brute-force)

- Training data was collected by moving the robot in environments such as kitchens (three types) and tables, as shown in Figure 2.
    - Remote control by a human
    - Humans provide textual commentary on the tasks performed.
        - This creates a set of images, text, and actions.
- Collected training data
    - 744 tasks (referred to as "skills" in the paper) (Table 1 in the paper)
    - 13 robots
    - 130,000 episodes

---
### Universal Sentence Encoder ([[Cer 2018]](https://arxiv.org/abs/1803.11175))

(This seems important, but just an overview)

- Convert sentences into vectors
    - Make the dot product of similar sentences larger
- Structure: Transformer (encoder) or Deep Average Network Encoder ([[Iyyer 2015]](https://aclanthology.org/P15-1162/))
- Learning method
    - Predict previous and following sentences
    - Predict response sentences to questions
    - Check whether premise and hypothesis statements are contradictory

---

### The part before the Transformer

- Two parts: FiLM EfficientNet-B3 and TokenLearner
    - FiLM EfficientNet-B3: Convert each image to tokens
        - Extract image features using a network called EfficientNet
            - Transform words using FiLM to add emphasis to features
        - For each image, it outputs 81 vectors (vision-language tokens) with 512 dimensions.
    - TokenLearner [[Ryoo 2021]](https://research.google/pubs/tokenlearner-adaptive-space-time-tokenization-for-videos/)
        - Reduces (compresses) the number of tokens.
            - Originally designed to reduce the number of input vectors for ViT.
        - $81\rightarrow8$ (48 tokens for 6 images, 512 dimensions).

---

### Transformer part

- 8 self-attention layers, 19 million parameters
- 8 tokens from each of the 6 images are arranged in order to create a sentence-like input.
    - This is converted into time-series information indicating what to do.
- Output: As mentioned above, parameters and modes for the dimensions required to move the robot.
    - Modes: arm, base, terminate
- Learning from training data
    - Predicting the next step based on verbal instructions and images.
        - It appears to learn using the decoder's mask function.
(Note: The entire model trains, not just the Transformer.)

---

### RT-1 Achievements

- Achieved 97% success rate on the trained task type
    - Example of behavior: Figure 5 in the paper
- Robustness
    - Tasks in a different kitchen than the trained one
    - Various tablecloths
- Support for longer/more abstract tasks than the trained one
    - Examples
        - "How would you throw away all the items on the table?"
        - Indirect location specifications, such as "near a sink"
- Other
    - Use of training data from simulations or different robots

---

### PaLM-E [[Driess2023]](https://arxiv.org/abs/2303.03378) ([Video](https://research.google/blog/palm-e-an-embodied-multimodal-language-model/))

- Embodied multimodal language model
    - Embodiment: Incorporating information from the robot's body
- A model that allows PaLM to input images and robot sensory information, making it enable to compose instructions on how to perform a task.
    - Example (taken from Figure 1 in the paper)
        - Input: "Given `<img>` Task: Sort colors into corners."
            - `<img>`: Image (in this case, various colored objects on a desk)
        - Output: "Step 1. Push the green star to the bottom left. Step 2. Push the green circle to the green star."
- Difference from RT-1: <span style="color:red">PaLM can utilize its linguistic knowledge</span>
    - It can also answer questions such as "What's in the image?" and "What happened between this image and this image?" regardless of the action.

---

### PaLM-E (PaLM-E-562B) Configuration (Figure 1 in the paper) (Part 1)

- Input to PaLM
    - ViT (22 billion parameters): Converts images to tokens for PaLM
    - "?": Converts other information to tokens for PaLM
        - This varies depending on the input type, so it's probably written as "?"
            - Basically, anything that can output a token is fine
- Examples of information to input to "?"
    - Sensor values other than images
    - Labeling, naming, and position estimation results for objects in images (using [[Sajjadi2022]](https://arxiv.org/abs/2206.06922), etc.)
    - Various other inputs are also possible

---

### PaLM-E (PaLM-E-562B) Structure (Figure 1 in the paper) (Part 2)

- Main Unit
    - PaLM (540 billion parameters): Accepts words as tokens
        - Output: Compositions that take into account the robot's physical structure
             - Outputs instructions
- Controller: To move the robot, the purple part in the diagram is required
    - [[Lynch2020]](https://arxiv.org/abs/2005.07648) or RT-1, etc.
    - If you want to directly output actuator instructions to PaLM, you can train it in this way, but it doesn't seem to be able to generate very long sequences (understanding of the lecture based on the paper).

---

### PaLM-E Training Method

- Training Data (Table 6 in the paper)
    - Sets of sentences and other data
        - Sentences: Links of questions and answers
             - A value indicating up to which token the task content is included is also prepared.
        - Image and sensor data
            - A fixed number of sets are input in a time series.
- Loss function: Cross-entropy error between answer and output.
- Notes
    - <span style="color:red">Most of the data are about answering image content, with less than 10% being robot-related.</span>
        - More than half are from the [Webli]([[Chen2022]](https://arxiv.org/html/2410.23676v1)) dataset.

---

### Robot-related training data (3 types)

- Task and Motion Planning (TAMP): B.1 of the paper
    - Answering questions about blocks of various colors on a desk in a simulator environment
        - Object relationships and Planning
- Language-Table: B.2 of the paper (data from [[Lynch2022]](https://arxiv.org/abs/2210.06407))
    - A set of videos and explanatory text showing the transition of manipulator joint angles and the state of work.
    - 600,000 sets were created in [Lynch2022].
- Kitchen environment dataset.
    - To compensate for the fact that the above two only see building blocks on a desk.
    - Similar to the training data in [[Ahn2022]](https://arxiv.org/abs/2204.01691).

---

## ALOHA, ACT [[Zhao2023]](https://arxiv.org/abs/2304.13705)

- Proposal of a framework for imitation learning.
    - Basically, it learns one task.
- ALOHA: A Low-cost Open-source Hardware System for Bimanual Teleoperation.
    - A teleoperation system for acquiring training data.
        - [Training Data Acquisition](https://youtu.be/VOpTZBwN7xs?si=IT14vfsjEirvKfKF&t=52)
        - [Example of Items Available for Purchase](https://www.tegtks.net/products/case10.html)
        - [Mobile Manipulator Version](https://www.youtube.com/watch?v=zMNumQ45pJ8)
- ACT: Next Page

---

### ACT: Action Chunking with Transformers

- Model that outputs control in continuous quantities rather than discrete quantities
- Outputs several steps ahead at once, rather than one step at a time, increasing the control period and achieving smoother control
- Example of Robot Movement
    - [Opening the Lid of a Small Cup](https://youtu.be/VOpTZBwN7xs?si=9tmS5TD94stPVUOF&t=214)
    - [Battery insertion, etc.](https://youtu.be/VOpTZBwN7xs?si=7zIrDrnEHrqJYWfx&t=252)
    - https://www.youtube.com/watch?v=VUxFhtGWD7w

---

### ACT Calculation (Training)

Configuration: Conditional VAE (CVAE) created with Transformer

- Encoder distribution: $q_\phi(\boldsymbol{z}|\boldsymbol{a}_{t:t+k},$ non-image sensor value at time $t)$
    - $\boldsymbol{z}$: Vector in latent space (called style variable, described below)
    - $\boldsymbol{a}_{t:t+k}$: Action sequence from time $t$ to $t+k$ (with positional embedding)
        - "Non-image": Internal sensors, etc. Removing images saves time.
- Distribution generated by the decoder: $\pi_\phi(\hat{\boldsymbol{a}}_{t:t+k} |\boldsymbol{z},$ sensor values ​​at $t$, including image $)$
    - $\hat{\boldsymbol{a}}_{t:t+k}$: Recovered motion sequence
$\qquad\qquad\qquad$![w:500](./figs/act_enc_dec.svg)

---

### Additional information on training

- Role of style variables and latent space
    - Since there are multiple robot movements (modes) for the same task, this prevents mixing of these.
        - The encoder assigns various modes to the Gaussian distribution in the latent space.
            - The Gaussian distribution is a simple one with zero covariance.
        - The decoder assigns $\boldsymbol{z} = 0 \rightarrow$ Specifying the center action sequence in the latent space.
            - Conditioning on image and sensor values yields appropriate action for the task.

---

- Loss function: L1 error (sum of absolute errors) of $\hat{\boldsymbol{a}}_{t:t+k}$.
    - L1 error: Used to definitively select the best action rather than the average optimal (e.g., identifying individuals from images).
        - Learned in optimization lectures.
        - Use in ACT: It seems that they want to output a solid action.

---

### ACT calculation (use/inference)

- As explained on the previous page, use only the decoder as follows:
    - $\hat{\boldsymbol{a}}_{t:t+k} \sim \pi_\phi(\boldsymbol{z} = \boldsymbol{0},$ sensor values including image$)$
- Smoothing of $\hat{\boldsymbol{a}}_{t:t+k}$
    - The decoder outputs the action sequence again before it finishes.
     $\rightarrow$Weighted average is calculated and input to the actuator.

![bg right:25% 100%](./figs/act_dec_use.svg)

---

### ACT Implementation

- Figure 4 in [Paper](https://arxiv.org/abs/2304.13705)
- CVAE Encoder: "BART-like" Transformer Encoder
    - Attach `[CLS]` to the input, and set the output of `[CLS]` to $\boldsymbol{z}$
- CVAE Decoder: Transformer Encoder-Decoder Configuration
    - The decoder uses an encoder, which is confusing.
    - Input to the Transformer Encoder
        - Four camera images, 14 joint angles for two manipulators, and $\boldsymbol{z}$
            - See the next page for details.
    - Input to the Transformer Decoder: Position embedding only
        - <span style="color:red">Reflecting the Transformer encoder output using a cross-attention mechanism</span>
    - Transformer decoder output: $\hat{\boldsymbol{a}}_{t:t+k}$ ($k$ vectors of $14$ dimension)

---

### ACT Implementation (Details of the Transformer Encoder Input)

- Camera image
    - RGB image of $480\times640$ pixels from four directions
        - 512-dimensional 300 vectors per image are processed using ResNet
             - Total of $1200$ vectors
- 14 joint angles
    - Combined into a single $512$-dimensional vector
- Style variable $\boldsymbol{z}$
    - This is also a single $512$-dimensional vector

<center>So far, 512-dimensional 1202 vectors have been input</center>

---

## Summary

- Robots can now perform tasks according to natural language.
    - RT-1: Robot movement generation
    - PaLM-E: Task planning at a higher level than RT-1
    - By combining with the vision-language model, they can adapt to new situations.
- Robots can now perform detailed tasks.
   - ACT
- What we haven't yet achieved with this content.
    - How can we enable multiple types of robots to perform multiple tasks?
