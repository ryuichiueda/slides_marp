---
marp: true
---

<!-- footer: "Advanced Vision Lesson 9" -->

# Advanced Vision

## Lesson 9: Integration of Images, Language, and Robot Control I

Ryuichi Ueda, Chiba Institute of Technology

<br />

<span style="font-size:70%">This work is licensed under a </span>[<span style="font-size:70%">Creative Commons Attribution-ShareAlike 4.0 International License</span>](https://creativecommons.org/licenses/by-sa/4.0/).
![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)

---

<!-- paginate: true -->

## Contents

- Before the main topic
   - PaLM
   - PaLI
- Robot Control using ANNs
    - models before the term VLA (vision-language-action model) appears
        - We'll discuss what happened after the term VLA appeared next time.

---

## PaLM[[Chowdhery2022]](https://arxiv.org/abs/2204.02311)

- Pathways Language Model (Google's large-scale language model)
    - It's composed of a Transformer decoder, so it functions like GPT.
    - As described below, it's used in Google's robot control models.
    - The new version, PaLM 2[[Anil2023]](https://arxiv.org/abs/2305.10403), can handle over 100 languages (multilingual translation is possible).

---
## PaLI[[Chen2022]](https://arxiv.org/abs/2209.06794)

- Pathways Language and Image model
    - Structure: Figure 1 in the paper (ViT + Transformer encoder + Transformer decoder)
        - Word tokens and image feature tokens generated by ViT are input into the Transformer as vectors of the same length.
        - It appears that a cross-attention mechanism is also used (sorry, not investigated).

---

## Controlling a robot using an ANN

---

### Basic concept

- Building an ANN that converts various types of information into motion.
    - Images, sensors
    - Verbal instructions
    - Its own internal state and structure
- Goal: extending the existing vision-language model
    - To enable input of information other than images
    - To enable output of motion

---

### Challenges of controlling a robot using an ANN

- How can we collect training data?
    - There probably isn't any online data like there was with CLIP.
    - <span style="color:red">Humans hired by big companies prepare the data. (It's a kind of distopia?)</span>
    - Of course, a simulator (digital twin) can also be used.
        - Examples of the two methods above: https://www.youtube.com/watch?v=S4tvirlG8sQ
- How to express and output policies (control rules)
    - how to design the format
    - how to output long control sequences

---

- Generalization
    - Able to perform actions outside of those taught
    - Possibly enabled by the presence of language (understanding of meaning)

---

### Robotics Transformer-1 (RT-1) [[Brohan 2022]](https://arxiv.org/abs/2212.06817) ([Video](https://www.youtube.com/watch?v=UuKAp9a6wMs))

- Overview of architecture and input/output: Figure 3 in the paper
    - Universal Sentence Encoder: Outputs parameters from [FiLM](lesson8.html#3)
    - FiLM EfficientNet-B3 and TokenLearner
        - Input: Six time-series images and verbal instructions to the robot
        - Output: 48 tokens with 512 dimensions
- Transformer (decoder)
    - Input: Positional embeddings for the 48 tokens
    - Output: Robot behavior (points in 11-dimensional discrete space)
        - Robot: Mobile manipulator ([Everyday Robots](https://x.company/projects/everyday-robots/))
        - 1-dimensional mode, 7-dimensional arm movement, 3-dimensional position and orientation
        - 3 Hz

---
### RT-1 training data (brute-force)

- Training data was collected by moving the robot in environments such as kitchens (three types) and tables, as shown in Figure 2.
    - Remote control by a human
    - Humans provide textual commentary on the tasks performed.
        - This creates a set of images, text, and actions.
- Collected training data
    - 744 tasks (referred to as "skills" in the paper) (Table 1 in the paper)
    - 13 robots
    - 130,000 episodes

---
### Universal Sentence Encoder ([[Cer 2018]](https://arxiv.org/abs/1803.11175))

(This seems important, but just an overview)

- Convert sentences into vectors
    - Make the dot product of similar sentences larger
- Structure: Transformer (encoder) or Deep Average Network Encoder ([[Iyyer 2015]](https://aclanthology.org/P15-1162/))
- Learning method
    - Predict previous and following sentences
    - Predict response sentences to questions
    - Check whether premise and hypothesis statements are contradictory

---

### The part before the Transformer

- Two parts: FiLM EfficientNet-B3 and TokenLearner
    - FiLM EfficientNet-B3: Convert each image to tokens
        - Extract image features using a network called EfficientNet
            - Transform words using FiLM to add emphasis to features
        - For each image, it outputs 81 vectors (vision-language tokens) with 512 dimensions.
    - TokenLearner [[Ryoo 2021]](https://research.google/pubs/tokenlearner-adaptive-space-time-tokenization-for-videos/)
        - Reduces (compresses) the number of tokens.
            - Originally designed to reduce the number of input vectors for ViT.
        - $81\rightarrow8$ (48 tokens for 6 images, 512 dimensions).

---

### Transformer part

- 8 self-attention layers, 19 million parameters
- 8 tokens from each of the 6 images are arranged in order to create a sentence-like input.
    - This is converted into time-series information indicating what to do.
- Output: As mentioned above, parameters and modes for the dimensions required to move the robot.
    - Modes: arm, base, terminate
- Learning from training data
    - Predicting the next step based on verbal instructions and images.
        - It appears to learn using the decoder's mask function.
(Note: The entire model trains, not just the Transformer.)

---

### RT-1 Achievements

- Achieved 97% success rate on the trained task type
    - Example of behavior: Figure 5 in the paper
- Robustness
    - Tasks in a different kitchen than the trained one
    - Various tablecloths
- Support for longer/more abstract tasks than the trained one
    - Examples
        - "How would you throw away all the items on the table?"
        - Indirect location specifications, such as "near a sink"
- Other
    - Use of training data from simulations or different robots

---

### PaLM-E [[Driess2023]](https://arxiv.org/abs/2303.03378) ([Video](https://research.google/blog/palm-e-an-embodied-multimodal-language-model/))

- Embodied multimodal language model
    - Embodiment: Incorporating information from the robot's body
- A model that allows PaLM to input images and robot sensory information, making it enable to compose instructions on how to perform a task.
    - Example (taken from Figure 1 in the paper)
        - Input: "Given `<img>` Task: Sort colors into corners."
            - `<img>`: Image (in this case, various colored objects on a desk)
        - Output: "Step 1. Push the green star to the bottom left. Step 2. Push the green circle to the green star."
- Difference from RT-1: <span style="color:red">PaLM can utilize its linguistic knowledge</span>
    - It can also answer questions such as "What's in the image?" and "What happened between this image and this image?" regardless of the action.

---

### PaLM-E (PaLM-E-562B) Configuration (Figure 1 in the paper) (Part 1)

- Input to PaLM
    - ViT (22 billion parameters): Converts images to tokens for PaLM
    - "?": Converts other information to tokens for PaLM
        - This varies depending on the input type, so it's probably written as "?"
            - Basically, anything that can output a token is fine
- Examples of information to input to "?"
    - Sensor values other than images
    - Labeling, naming, and position estimation results for objects in images (using [[Sajjadi2022]](https://arxiv.org/abs/2206.06922], etc.)
    - Various other inputs are also possible

---
### PaLM-E (PaLM-E-562B) Configuration (Figure 1 in [Paper](https://arxiv.org/abs/2303.03378)) (Part 2)

- Main Unit
- PaLM (540 billion parameters): Accepts words as tokens
- Output: Essays that take into account embodiment (the robot's own body structure)
